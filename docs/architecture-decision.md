# Solution QA System - Architecture Decision

**Date:** January 28, 2026  
**Status:** Proposed  
**Author:** Solution QA Team

---

## Overview

This document compares different approaches for building an LLM-based Solution QA system and provides a recommendation.

---

## Requirements

| Requirement | Description |
|-------------|-------------|
| **Code Understanding** | Analyze and assess solution implementation code |
| **PostgreSQL Queries** | Read and validate data from databases |
| **Performance Testing** | Measure load times and UI responsiveness |
| **Quality Scoring** | Score and report solution quality |
| **LLM-Based** | Use LLM for reasoning and decision-making |

---

## Options Evaluated

### Option 1: Codex CLI + MCP

OpenAI's agentic CLI tool that runs locally, connects to MCP servers for tools, and executes tasks autonomously.

**Pros:**
- Agentic by design â€” built for autonomous multi-step tasks with tool calling
- First-class MCP server integration
- Simple orchestration â€” just shell out to `codex exec --full-auto`
- Local execution â€” code runs locally, can access files/DBs directly
- Native code understanding

**Cons:**
- Limited control â€” black-box execution, hard to customize agent behavior
- Debugging â€” difficult to debug or trace reasoning
- Coupling â€” tight coupling to OpenAI's tool/model

---

### Option 2: OpenAI SDK Framework

Building custom agents using OpenAI's Python SDK (including the Agents SDK/Swarm patterns).

**Pros:**
- Full control over agent loop, retry logic, scoring
- Transparency â€” can log every decision, tool call, and response
- Custom scoring â€” build your own quality scoring rubrics
- Multi-model â€” can mix models (cheap for routing, powerful for analysis)
- Tool flexibility â€” define tools as Python functions directly

**Cons:**
- More code â€” need to implement agent loop, context management, error handling
- No native code understanding â€” needs file reading tools or code indexing solution
- Vendor lock-in â€” tied to OpenAI (though can abstract)

---

### Option 3: Copilot SDK / GitHub Copilot Extensions

Building on GitHub Copilot's infrastructure via extensions or the emerging Copilot SDK.

**Pros:**
- Native code understanding â€” deep IDE/codebase integration
- VS Code integration â€” can leverage editor context
- GitHub ecosystem â€” works with PRs, issues, Actions

**Cons:**
- Limited SDK availability â€” Copilot SDK is still emerging/restricted
- IDE dependency â€” designed for interactive use, not batch QA
- Less control â€” harder to run headless/CI pipelines
- Tool limitations â€” not designed for DB queries, perf testing
- No Azure OpenAI support

---

### Option 4: LangChain/LangGraph

Popular framework for building LLM applications with agent orchestration.

**Pros:**
- Model agnostic â€” easy to swap OpenAI â†” Azure â†” Anthropic â†” local
- Rich tooling â€” built-in tools for SQL, web, code execution
- State management â€” LangGraph provides stateful agent graphs
- Community â€” large ecosystem of integrations

**Cons:**
- Abstraction overhead â€” can be over-engineered for simpler tasks
- Learning curve â€” many concepts to learn
- Debugging â€” chains can be hard to trace

---

## Comparison Matrix

| Criteria | Codex CLI + MCP | OpenAI SDK | Copilot SDK | LangChain |
|----------|-----------------|------------|-------------|-----------|
| **Code understanding** | â­â­â­â­â­ Native | â­â­ Needs tools | â­â­â­â­ Native | â­â­â­ Needs tools |
| **PostgreSQL tools** | â­â­â­â­ Via MCP | â­â­â­â­â­ Direct | â­â­ Limited | â­â­â­â­ Built-in |
| **Playwright/UI tools** | â­â­â­â­ Via MCP | â­â­â­â­â­ Direct | â­â­ Limited | â­â­â­â­ Built-in |
| **Quality scoring** | â­â­â­ Parse output | â­â­â­â­â­ Structured | â­â­â­ Parse output | â­â­â­â­ Structured |
| **Autonomous execution** | â­â­â­â­â­ Full auto | â­â­â­ Manual loop | â­â­ Interactive | â­â­â­â­ Agent mode |
| **Control & debugging** | â­â­ Black box | â­â­â­â­â­ Full control | â­â­â­ Moderate | â­â­â­ Moderate |
| **CI/CD friendly** | â­â­â­â­ Yes | â­â­â­â­â­ Yes | â­â­ IDE-focused | â­â­â­â­ Yes |
| **Implementation effort** | â­â­â­â­â­ Low (existing) | â­â­â­ Medium | â­â­ High | â­â­â­ Medium |
| **Azure OpenAI support** | â­â­â­â­â­ Yes | â­â­â­â­â­ Yes | â­â­ No | â­â­â­â­â­ Yes |

---

## Recommendation: Codex CLI + MCP (Unified Approach)

### Why This Approach?

| Reason | Explanation |
|--------|-------------|
| âœ… **Already built** | We have working MCP infrastructure and orchestrator |
| âœ… **Best code understanding** | Codex natively understands codebases, no extra tooling needed |
| âœ… **Extensible via MCP** | Add PG, Playwright, scoring tools to existing server |
| âœ… **Autonomous** | `--full-auto` handles multi-step reasoning without custom agent loop |
| âœ… **Single architecture** | One system to maintain, not two |
| âœ… **Azure compatible** | Already configured for Azure OpenAI |

### Why Not the Alternatives?

| Alternative | Why Not |
|-------------|---------|
| **OpenAI SDK** | Loses native code understanding, requires more code |
| **Copilot SDK** | Not designed for batch/CI QA, no Azure support |
| **LangChain** | Abstraction overhead, Codex already provides agent loop |
| **Hybrid** | Adds complexity without significant benefit |

---

## Recommended Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Solution QA Orchestrator (Python)             â”‚
â”‚                   agents/solution_qa.py                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Codex CLI (--full-auto)                   â”‚
â”‚                    codex/config.toml                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     MCP Server (FastMCP)                   â”‚
â”‚                   mcp_server/mcp_server.py                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ğŸ“Š Risk Model Tools        (existing)                     â”‚
â”‚  ğŸ—„ï¸ PostgreSQL Tools        (to add)                       â”‚
â”‚  ğŸ­ Playwright/Perf Tools   (to add)                       â”‚
â”‚  ğŸ“ Scoring & Report Tools  (to add)                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Implementation Plan

| Phase | Tools to Add | Purpose |
|-------|--------------|---------|
| **Phase 1** âœ… | `read_risk_model_raw`, `validate_risk_model`, etc. | Risk model QA |
| **Phase 2** | `query_postgres`, `list_tables`, `validate_data` | Data validation |
| **Phase 3** | `measure_load_time`, `check_ui_element`, `capture_screenshot` | Performance QA |
| **Phase 4** | `score_feature`, `generate_report`, `compare_with_baseline` | Scoring & reporting |

---

## Decision Summary

| Decision | Choice |
|----------|--------|
| **Architecture** | Codex CLI + MCP (unified) |
| **Why not hybrid?** | Adds complexity without significant benefit |
| **Why not OpenAI SDK?** | Loses native code understanding, more code to write |
| **Why not Copilot SDK?** | Not designed for batch/CI QA, Azure not supported |
| **Why not LangChain?** | Abstraction overhead, Codex already provides agent loop |
| **Next step** | Add PostgreSQL and Playwright MCP tools |

---

## Open Questions

1. **CI/CD Integration** - Should this run in GitHub Actions or a dedicated QA environment?
2. **Scoring Rubrics** - What specific quality metrics should we score?
3. **Baseline Comparison** - Do we need to compare against previous QA runs?
4. **Alerting** - Should failures trigger Slack/email notifications?

---

## References

- [Codex CLI Documentation](https://github.com/openai/codex)
- [MCP Specification](https://modelcontextprotocol.io/)
- [FastMCP Library](https://github.com/jlowin/fastmcp)
- Internal: `solution-qa-code` repository
